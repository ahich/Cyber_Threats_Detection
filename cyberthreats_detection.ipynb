{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2967110-9515-4a1b-8ab6-f7bfd5c84d83",
   "metadata": {},
   "source": [
    "\n",
    "### The Data as organized by the preprocessing:\n",
    "\n",
    "| Column     | Description              |\n",
    "|------------|--------------------------|\n",
    "|`processId`|The unique identifier for the process that generated the event - int64 |\n",
    "|`threadId`|ID for the thread spawning the log - int64|\n",
    "|`parentProcessId`|Label for the process spawning this log - int64|\n",
    "|`userId`|ID of user spawning the log|Numerical - int64|\n",
    "|`mountNamespace`|Mounting restrictions the process log works within - int64|\n",
    "|`argsNum`|Number of arguments passed to the event - int64|\n",
    "|`returnValue`|Value returned from the event log (usually 0) - int64|\n",
    "|`sus_label`|Binary label as suspicous event (1 is suspicious, 0 is not) - int64|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35558034-b7b7-4ab7-8d59-5e6ed281f838",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 3168,
    "lastExecutedAt": 1728730236373,
    "lastExecutedByKernel": "c7568e17-f0a9-462f-9104-525cd42aa811",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Make sure to run this cell to use torchmetrics. If you cannot use pip install to install the torchmetrics, you can use sklearn.\n!pip install torchmetrics",
    "outputsMetadata": {
     "0": {
      "height": 457,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torchmetrics in /home/repl/.local/lib/python3.8/site-packages (1.4.3)\n",
      "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.8/dist-packages (from torchmetrics) (1.23.2)\n",
      "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.8/dist-packages (from torchmetrics) (23.2)\n",
      "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.8/dist-packages (from torchmetrics) (1.13.0)\n",
      "Requirement already satisfied: lightning-utilities>=0.8.0 in /home/repl/.local/lib/python3.8/site-packages (from torchmetrics) (0.11.7)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torchmetrics) (4.9.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (65.6.3)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.8/dist-packages (from torch>=1.10.0->torchmetrics) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.8/dist-packages (from torch>=1.10.0->torchmetrics) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.8/dist-packages (from torch>=1.10.0->torchmetrics) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.8/dist-packages (from torch>=1.10.0->torchmetrics) (11.7.99)\n",
      "Requirement already satisfied: wheel in /usr/local/lib/python3.8/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.10.0->torchmetrics) (0.38.4)\n"
     ]
    }
   ],
   "source": [
    "# Make sure to run this cell to use torchmetrics. If you cannot use pip install to install the torchmetrics, you can use sklearn.\n",
    "!pip install torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3121b05-9873-431d-812c-62bceffbf1b3",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 3178,
    "lastExecutedAt": 1728730239552,
    "lastExecutedByKernel": "c7568e17-f0a9-462f-9104-525cd42aa811",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Import required libraries\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as functional\nfrom torch.utils.data import DataLoader, TensorDataset\nimport torch.optim as optim\nfrom torchmetrics import Accuracy\n# from sklearn.metrics import accuracy_score  # uncomment to use sklearn"
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as functional\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.optim as optim\n",
    "from torchmetrics import Accuracy\n",
    "# from sklearn.metrics import accuracy_score  # uncomment to use sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08bfe2bf-3132-490e-9c16-9026f82b8d73",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 513,
    "lastExecutedAt": 1728730240067,
    "lastExecutedByKernel": "c7568e17-f0a9-462f-9104-525cd42aa811",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Load preprocessed data\ntrain_df = pd.read_csv('labelled_train.csv')\ntest_df = pd.read_csv('labelled_test.csv')\nval_df = pd.read_csv('labelled_validation.csv')\n\n# View the first 5 rows of training set\ntrain_df.head()",
    "outputsMetadata": {
     "0": {
      "height": 237,
      "type": "dataFrame"
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/com.datacamp.data-table.v2+json": {
       "table": {
        "data": {
         "argsNum": [
          5,
          1,
          0,
          2,
          4
         ],
         "index": [
          0,
          1,
          2,
          3,
          4
         ],
         "mountNamespace": [
          4026532231,
          4026532231,
          4026532231,
          4026531840,
          4026531840
         ],
         "parentProcessId": [
          1,
          1,
          1,
          7341,
          7341
         ],
         "processId": [
          381,
          381,
          381,
          7347,
          7347
         ],
         "returnValue": [
          0,
          0,
          0,
          -2,
          0
         ],
         "sus_label": [
          1,
          1,
          1,
          1,
          1
         ],
         "threadId": [
          7337,
          7337,
          7337,
          7347,
          7347
         ],
         "userId": [
          100,
          100,
          100,
          0,
          0
         ]
        },
        "schema": {
         "fields": [
          {
           "name": "index",
           "type": "integer"
          },
          {
           "name": "processId",
           "type": "integer"
          },
          {
           "name": "threadId",
           "type": "integer"
          },
          {
           "name": "parentProcessId",
           "type": "integer"
          },
          {
           "name": "userId",
           "type": "integer"
          },
          {
           "name": "mountNamespace",
           "type": "integer"
          },
          {
           "name": "argsNum",
           "type": "integer"
          },
          {
           "name": "returnValue",
           "type": "integer"
          },
          {
           "name": "sus_label",
           "type": "integer"
          }
         ],
         "pandas_version": "1.4.0",
         "primaryKey": [
          "index"
         ]
        }
       },
       "total_rows": 5,
       "truncation_type": null
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>processId</th>\n",
       "      <th>threadId</th>\n",
       "      <th>parentProcessId</th>\n",
       "      <th>userId</th>\n",
       "      <th>mountNamespace</th>\n",
       "      <th>argsNum</th>\n",
       "      <th>returnValue</th>\n",
       "      <th>sus_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>381</td>\n",
       "      <td>7337</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>4026532231</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>381</td>\n",
       "      <td>7337</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>4026532231</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>381</td>\n",
       "      <td>7337</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>4026532231</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7347</td>\n",
       "      <td>7347</td>\n",
       "      <td>7341</td>\n",
       "      <td>0</td>\n",
       "      <td>4026531840</td>\n",
       "      <td>2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7347</td>\n",
       "      <td>7347</td>\n",
       "      <td>7341</td>\n",
       "      <td>0</td>\n",
       "      <td>4026531840</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   processId  threadId  parentProcessId  ...  argsNum  returnValue  sus_label\n",
       "0        381      7337                1  ...        5            0          1\n",
       "1        381      7337                1  ...        1            0          1\n",
       "2        381      7337                1  ...        0            0          1\n",
       "3       7347      7347             7341  ...        2           -2          1\n",
       "4       7347      7347             7341  ...        4            0          1\n",
       "\n",
       "[5 rows x 8 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load preprocessed data\n",
    "train_df = pd.read_csv('labelled_train.csv')\n",
    "test_df = pd.read_csv('labelled_test.csv')\n",
    "val_df = pd.read_csv('labelled_validation.csv')\n",
    "\n",
    "# View the first 5 rows of training set\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f8e783-359a-4ee0-8229-1c3e90575a06",
   "metadata": {},
   "source": [
    "## We can see that the features have different scales, a standardization is thus needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8b264e9-bbf0-40ec-9b21-6c167220bb61",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 74,
    "lastExecutedAt": 1728730240141,
    "lastExecutedByKernel": "c7568e17-f0a9-462f-9104-525cd42aa811",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Start coding here\n# Use as many cells as you need\n#The features have very different scales and need to go standard scaler:\ntrain_features = train_df[train_df.columns[1:-1]]\ntrain_labels = train_df[train_df.columns[-1]].to_numpy()\ntest_features = test_df[test_df.columns[1:-1]]\ntest_labels = test_df[test_df.columns[-1]].to_numpy()\nval_features = val_df[val_df.columns[1:-1]]\nval_labels = val_df[val_df.columns[-1]].to_numpy()\n\n\nscaler = StandardScaler()\ntrain_features = scaler.fit_transform(train_features)\ntest_features = scaler.transform(test_features)\nval_features = scaler.transform(val_features)\n\n\n\nval_features[:5]",
    "outputsMetadata": {
     "0": {
      "height": 414,
      "type": "dataFrame"
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.32423454, -0.84909243,  2.63843038,  1.79005541,  0.24455003,\n",
       "        -0.00791259],\n",
       "       [-3.32578327, -0.84909243,  2.61170424,  1.78399123,  0.24455003,\n",
       "        -0.00791259],\n",
       "       [-3.52040741, -0.84954378, -0.06090978, -0.58710151,  0.99031479,\n",
       "        -0.0549941 ],\n",
       "       [-3.52040741, -0.84954378, -0.06090978, -0.58710151,  0.99031479,\n",
       "        -0.01732889],\n",
       "       [-3.52040741, -0.84954378, -0.06090978, -0.58710151, -0.50121474,\n",
       "        -0.0549941 ]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start coding here\n",
    "# Use as many cells as you need\n",
    "#The features have very different scales and need to go standard scaler:\n",
    "train_features = train_df[train_df.columns[1:-1]]\n",
    "train_labels = train_df[train_df.columns[-1]].to_numpy()\n",
    "test_features = test_df[test_df.columns[1:-1]]\n",
    "test_labels = test_df[test_df.columns[-1]].to_numpy()\n",
    "val_features = val_df[val_df.columns[1:-1]]\n",
    "val_labels = val_df[val_df.columns[-1]].to_numpy()\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train_features = scaler.fit_transform(train_features)\n",
    "test_features = scaler.transform(test_features)\n",
    "val_features = scaler.transform(val_features)\n",
    "\n",
    "\n",
    "\n",
    "val_features[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f26b6fc-2947-496a-992b-4a72f160de4d",
   "metadata": {},
   "source": [
    "## Build data loaders but later, due to the small size of the datasets, they won't be needed and the training sample can be passed as a unique **batch:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5821f507-cbf5-49b1-8073-cb3fa5995357",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 53,
    "lastExecutedAt": 1728730240194,
    "lastExecutedByKernel": "c7568e17-f0a9-462f-9104-525cd42aa811",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "#transform features and labels to torch tensors:\n\ntrain_features_tensor = torch.tensor(train_features, dtype=torch.float32)\ntrain_labels_tensor = torch.tensor(train_labels, dtype=torch.float32).view(-1, 1)\ntest_features_tensor = torch.tensor(test_features, dtype=torch.float32)\ntest_labels_tensor = torch.tensor(test_labels, dtype=torch.float32).view(-1, 1)\nval_features_tensor = torch.tensor(val_features, dtype=torch.float32)\nval_labels_tensor = torch.tensor(val_labels, dtype=torch.float32).view(-1, 1)\n\n#use TensorDatasets and DataLoaders. Won't be used later due to the simpleness of the data\ndataset_train = TensorDataset(train_features_tensor,train_labels_tensor)\ndataset_test = TensorDataset(test_features_tensor,test_labels_tensor)\ndataset_val =TensorDataset(val_features_tensor,val_labels_tensor)\n\ntrain_dataloader = DataLoader(\n    dataset_train, shuffle=True, batch_size=1,\n)\ntest_dataloader = DataLoader(\n    dataset_test, shuffle=True, batch_size=1,\n)\nval_dataloader = DataLoader(\n    dataset_val, shuffle=True, batch_size=1,\n)"
   },
   "outputs": [],
   "source": [
    "#transform features and labels to torch tensors:\n",
    "\n",
    "train_features_tensor = torch.tensor(train_features, dtype=torch.float32)\n",
    "train_labels_tensor = torch.tensor(train_labels, dtype=torch.float32).view(-1, 1)\n",
    "test_features_tensor = torch.tensor(test_features, dtype=torch.float32)\n",
    "test_labels_tensor = torch.tensor(test_labels, dtype=torch.float32).view(-1, 1)\n",
    "val_features_tensor = torch.tensor(val_features, dtype=torch.float32)\n",
    "val_labels_tensor = torch.tensor(val_labels, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "#use TensorDatasets and DataLoaders. Won't be used later due to the simpleness of the data\n",
    "dataset_train = TensorDataset(train_features_tensor,train_labels_tensor)\n",
    "dataset_test = TensorDataset(test_features_tensor,test_labels_tensor)\n",
    "dataset_val =TensorDataset(val_features_tensor,val_labels_tensor)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    dataset_train, shuffle=True, batch_size=1,\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    dataset_test, shuffle=True, batch_size=1,\n",
    ")\n",
    "val_dataloader = DataLoader(\n",
    "    dataset_val, shuffle=True, batch_size=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf7c24e-a2ab-46e4-b49a-35a3d8efd46e",
   "metadata": {},
   "source": [
    "## Building a simple model: ReLU/ELU activation functions were tried but happen to only slow/disturb the training. Removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08f45071-dd32-4f25-963f-99fa27498224",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 48,
    "lastExecutedAt": 1728730240242,
    "lastExecutedByKernel": "c7568e17-f0a9-462f-9104-525cd42aa811",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "#Build our model\nnum_features = train_features_tensor.shape[1]\nmodel = nn.Sequential(nn.Linear(num_features,256),\n                      nn.Linear(256,128),\n                      nn.Linear(128,1),\n                      nn.Sigmoid()\n                     )"
   },
   "outputs": [],
   "source": [
    "#Build our model\n",
    "num_features = train_features_tensor.shape[1]\n",
    "model = nn.Sequential(nn.Linear(num_features,256),\n",
    "                      nn.Linear(256,128),\n",
    "                      nn.Linear(128,1),\n",
    "                      nn.Sigmoid()\n",
    "                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5adee2ae-3466-4008-8331-e6d5f8178ff6",
   "metadata": {},
   "source": [
    "## Onto the training loop: use binary cross entropy as a loss function and Adam as an optimizer with a learning rate of 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19b3e17b-9b8f-4f63-847b-40d91eddf7c7",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 184372,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "lastExecutedAt": 1728730424614,
    "lastExecutedByKernel": "c7568e17-f0a9-462f-9104-525cd42aa811",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "#train:\n# Define the loss function\ncriterion = nn.BCELoss()\n# Define the optimizer\noptimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n\nnum_epochs = 20\n# Loop over the number of epochs and the dataloader\nfor epoch in range(num_epochs):\n    #running_loss = 0.\n    optimizer.zero_grad()\n    prediction = model(train_features_tensor)\n    loss = criterion(prediction, train_labels_tensor)\n    loss.backward()\n    optimizer.step()\n#    for data in train_dataloader:\n#        # Set the gradients to zero\n#        optimizer.zero_grad()\n#        # Run a forward pass\n#        feature, target = data\n#        #print(feature.to(torch.float32))\n#        prediction = model(feature)    \n#        #print(prediction.squeeze().size(), target.size())\n#        # Calculate the loss\n#        loss = criterion(prediction, target)    \n#        # Compute the gradients\n#        loss.backward()\n#        # Update the model's parameters\n#        optimizer.step()\n#        running_loss += loss.item()\n#    epoch_loss = running_loss / len(train_dataloader)\n    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n",
    "outputsMetadata": {
     "0": {
      "height": 457,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.6385\n",
      "Epoch 2, Loss: 0.5084\n",
      "Epoch 3, Loss: 0.3991\n",
      "Epoch 4, Loss: 0.3094\n",
      "Epoch 5, Loss: 0.2373\n",
      "Epoch 6, Loss: 0.1805\n",
      "Epoch 7, Loss: 0.1365\n",
      "Epoch 8, Loss: 0.1027\n",
      "Epoch 9, Loss: 0.0772\n",
      "Epoch 10, Loss: 0.0581\n",
      "Epoch 11, Loss: 0.0439\n",
      "Epoch 12, Loss: 0.0336\n",
      "Epoch 13, Loss: 0.0260\n",
      "Epoch 14, Loss: 0.0204\n",
      "Epoch 15, Loss: 0.0163\n",
      "Epoch 16, Loss: 0.0133\n",
      "Epoch 17, Loss: 0.0110\n",
      "Epoch 18, Loss: 0.0092\n",
      "Epoch 19, Loss: 0.0079\n",
      "Epoch 20, Loss: 0.0069\n"
     ]
    }
   ],
   "source": [
    "#train:\n",
    "# Define the loss function\n",
    "criterion = nn.BCELoss()\n",
    "# Define the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
    "\n",
    "num_epochs = 20\n",
    "# Loop over the number of epochs and the dataloader\n",
    "for epoch in range(num_epochs):\n",
    "    #running_loss = 0.\n",
    "    optimizer.zero_grad()\n",
    "    prediction = model(train_features_tensor)\n",
    "    loss = criterion(prediction, train_labels_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "#    for data in train_dataloader:\n",
    "#        # Set the gradients to zero\n",
    "#        optimizer.zero_grad()\n",
    "#        # Run a forward pass\n",
    "#        feature, target = data\n",
    "#        #print(feature.to(torch.float32))\n",
    "#        prediction = model(feature)    \n",
    "#        #print(prediction.squeeze().size(), target.size())\n",
    "#        # Calculate the loss\n",
    "#        loss = criterion(prediction, target)    \n",
    "#        # Compute the gradients\n",
    "#        loss.backward()\n",
    "#        # Update the model's parameters\n",
    "#        optimizer.step()\n",
    "#        running_loss += loss.item()\n",
    "#    epoch_loss = running_loss / len(train_dataloader)\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def2aabd-1d87-4dde-849f-c28579b672b0",
   "metadata": {},
   "source": [
    "## Evaluate the performance on the test and validation samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8640bc23-c023-4835-b692-7c4000d48428",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 992,
    "lastExecutedAt": 1728730425606,
    "lastExecutedByKernel": "c7568e17-f0a9-462f-9104-525cd42aa811",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "#estimate accuracy for test and valid samples:\naccu = Accuracy(threshold=0.5, task = 'binary')\n\ntest_preds = model(test_features_tensor)\n#val_preds = val_preds > 0.5\ntest_accuracy = accu(test_preds,test_labels_tensor).item()\nprint(\"test accuracy: \", test_accuracy)\nval_preds = model(val_features_tensor)\n#val_preds = val_preds > 0.5\nval_accuracy = accu(val_preds,val_labels_tensor).item()\nprint(\"val_accuracy: \", val_accuracy)",
    "outputsMetadata": {
     "0": {
      "height": 61,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy:  0.9459800124168396\n",
      "val_accuracy:  0.9999523758888245\n"
     ]
    }
   ],
   "source": [
    "#estimate accuracy for test and valid samples:\n",
    "accu = Accuracy(threshold=0.5, task = 'binary')\n",
    "\n",
    "test_preds = model(test_features_tensor)\n",
    "#val_preds = val_preds > 0.5\n",
    "test_accuracy = accu(test_preds,test_labels_tensor).item()\n",
    "print(\"test accuracy: \", test_accuracy)\n",
    "val_preds = model(val_features_tensor)\n",
    "#val_preds = val_preds > 0.5\n",
    "val_accuracy = accu(val_preds,val_labels_tensor).item()\n",
    "print(\"val_accuracy: \", val_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfb9903-3023-4b76-95d3-13b4bb44b2bc",
   "metadata": {},
   "source": [
    "## The accuracies are very high, let's see in terms of precision and recall:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b523402-b54e-4b15-9cd0-5e31fbc8842c",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 91,
    "lastExecutedAt": 1728730425697,
    "lastExecutedByKernel": "c7568e17-f0a9-462f-9104-525cd42aa811",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "from torchmetrics import Precision, Recall\nprecision = Precision(task=\"binary\", threshold=0.5)\nrecall = Recall(task=\"binary\", threshold=0.5)\n\ntest_precision = precision(test_preds,test_labels_tensor).item()\ntest_recall = recall(test_preds,test_labels_tensor).item()\n\nprint(\"test precision: \", test_precision,\"recall: \",test_recall)\n\nval_precision = precision(val_preds,val_labels_tensor).item()\nval_recall = recall(val_preds,val_labels_tensor).item()\n\nprint(\"validation precision: \", val_precision,\"recall: \",val_recall)\n\n",
    "outputsMetadata": {
     "0": {
      "height": 61,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test precision:  1.0 recall:  0.9404639005661011\n",
      "validation precision:  1.0 recall:  0.9885495901107788\n"
     ]
    }
   ],
   "source": [
    "from torchmetrics import Precision, Recall\n",
    "precision = Precision(task=\"binary\", threshold=0.5)\n",
    "recall = Recall(task=\"binary\", threshold=0.5)\n",
    "\n",
    "test_precision = precision(test_preds,test_labels_tensor).item()\n",
    "test_recall = recall(test_preds,test_labels_tensor).item()\n",
    "\n",
    "print(\"test precision: \", test_precision,\"recall: \",test_recall)\n",
    "\n",
    "val_precision = precision(val_preds,val_labels_tensor).item()\n",
    "val_recall = recall(val_preds,val_labels_tensor).item()\n",
    "\n",
    "print(\"validation precision: \", val_precision,\"recall: \",val_recall)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45189ab9-3db3-44cf-a8d8-8090fa007ac1",
   "metadata": {},
   "source": [
    "## The performance on these figures of merits is also very good!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Welcome to DataCamp Workspaces.ipynb",
   "provenance": []
  },
  "editor": "DataLab",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
